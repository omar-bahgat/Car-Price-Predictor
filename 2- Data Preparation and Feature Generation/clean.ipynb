{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 151 ms, total: 1.63 s\n",
      "Wall time: 1.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.read_csv(\"cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before dropping irrelevant rows: (762091, 20)\n",
      "After dropping irrelevant rows: (748211, 20)\n"
     ]
    }
   ],
   "source": [
    "# drop row when more than 25% of the data is missing\n",
    "print(\"Before dropping irrelevant rows:\", data.shape)\n",
    "data.dropna(thresh=14, inplace=True)\n",
    "print(\"After dropping irrelevant rows:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'data' with your actual data and feature name\n",
    "def visualize_feature_distribution_with_chart(data, feature_name,top_n):\n",
    "\n",
    "  # Get the value counts for the chosen feature\n",
    "  feature_counts = data[feature_name].value_counts()\n",
    "\n",
    "  # Create a DataFrame to store top n and \"Others\"\n",
    "  top_n_s = feature_counts.head(top_n)\n",
    "  top_n_s['Others'] = feature_counts[top_n:].sum()\n",
    "\n",
    "  # Print a table summarizing the distribution\n",
    "  print(f\"\\nDistribution of {feature_name} (Top {top_n} + Others):\")\n",
    "  print(top_n_s.to_string())\n",
    "\n",
    "  # Create the bar chart\n",
    "  plt.figure(figsize=(3, 2))\n",
    "  sns.barplot(x=top_n_s.index, y=top_n_s.values)\n",
    "  plt.title(f'Distribution of {feature_name} (Top {top_n} + Others)')\n",
    "  plt.xlabel(feature_name)\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n",
    "  plt.show()\n",
    "    \n",
    "    \n",
    "def feature_distribution(data, feature_name, top_n):\n",
    "\n",
    "    # Get the value counts for the chosen feature\n",
    "    feature_counts = data[feature_name].value_counts()\n",
    "\n",
    "    # Calculate total count\n",
    "    total_count = feature_counts.sum()\n",
    "\n",
    "    # Create a DataFrame to store top n and \"Others\"\n",
    "    top_n_s = feature_counts.head(top_n)\n",
    "    top_n_s['Others'] = feature_counts[top_n:].sum()\n",
    "\n",
    "    # Calculate percentages with formatting directly in the column\n",
    "    top_n_s['Percentage'] = (top_n_s / total_count) * 100\n",
    "    top_n_s['Percentage'] = top_n_s['Percentage'].astype(str) + '%'\n",
    "\n",
    "    # Print a table summarizing the distribution\n",
    "    print(f\"\\nDistribution of {feature_name} (Top {top_n} + Others):\")\n",
    "    print(top_n_s['Percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: 30\n",
      "Unique values of manufacturer: ['Acura' 'Audi' 'BMW' 'Buick' 'Cadillac' 'Chevrolet' 'Chrysler' 'Dodge'\n",
      " 'Ford' 'GMC' 'Honda' 'Hyundai' 'INFINITI' 'Jaguar' 'Jeep' 'Kia'\n",
      " 'Land Rover' 'Lexus' 'Lincoln' 'Mazda' 'Mercedes-Benz' 'Mitsubishi'\n",
      " 'Nissan' 'Porsche' 'RAM' 'Subaru' 'Tesla' 'Toyota' 'Volkswagen' 'Volvo']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature: manufacturer\n",
    "Approach:  No missing values, one-hot-encode.\n",
    "'''\n",
    "#get distribution\n",
    "#visualize_feature_distribution_with_chart(data, 'manufacturer',30) \n",
    "\n",
    "print(\"Unique values:\", len(data['manufacturer'].unique()))\n",
    "print(\"Unique values of manufacturer:\", data['manufacturer'].unique())\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data['manufacturer'], prefix='manufacturer')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('manufacturer', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values: 742\n",
      "Unique values of manufacturer: ['ILX' 'NSX' 'ZDX' 'RSX' 'Legend' 'MDX' 'RLX' 'Integra' 'TL' 'RDX' 'TSX'\n",
      " 'CL' 'TLX' 'RL' 'A8' 'RS' 'Q8' 'e-tron' 'R8' 'Q7' 'RS6' 'Q5' 'S5' 'S4'\n",
      " 'S3' 'A3' 'A4' 'SQ7' 'A5' 'TT' 'A6' 'TTS' 'Q3' 'S8' 'S6' 'Q4' 'S7' 'A7'\n",
      " 'SQ5' '228' '650' '530e' '745e' '640' '840' 'M8' 'X4' 'M6' '850' '430'\n",
      " '328' '128' '325' '428' 'ALPINA' '340' '323' '740' 'Z3' 'M' 'Z4' '528'\n",
      " 'M240' '440' '535' 'X3' 'X6' '330' 'X1' '550' '135' 'X7' 'i8' '645'\n",
      " 'M440' 'i7' 'M760' '435' 'M235' 'M340' 'M850' 'M3' 'M4' '230' '740e'\n",
      " '535d' 'M5' 'M2' 'i4' '745' 'Z8' '335' '525' '318' '2002' '320' '540'\n",
      " 'X2' '750' 'X5' '330e' '760' '530' 'ActiveHybrid' 'M550' '328d' 'iX' 'i3'\n",
      " 'Electra' 'Centurion' 'Enclave' 'GSX' 'Envision' 'Lucerne' 'Cascada'\n",
      " 'LeSabre' 'Encore' 'Riviera' 'Terraza' 'Century' 'Roadmaster' 'Super'\n",
      " 'Rendezvous' 'Regal' 'Skylark' 'Estate' 'Limited' 'Special' 'Verano'\n",
      " 'Model' 'Rainier' 'Reatta' 'Park' 'LaCrosse' 'GS' 'Eldorado' 'CT5-V'\n",
      " 'XT4' 'CTS-V' 'Escalade' 'ATS-V' 'XT5' 'CT6' 'DeVille' 'Series' 'Allante'\n",
      " 'Fleetwood' 'ELR' 'CT6-V' 'STS' 'SRX' 'CT4' 'ATS' 'Catera' 'CT5' 'XT6'\n",
      " 'XTS' 'DTS' 'XLR' 'Brougham' 'CT4-V' 'LYRIQ' 'Seville' 'CTS' 'Suburban'\n",
      " 'Bolt' 'Sedan' 'Venture' 'Uplander' 'El' 'Styleline' 'Corvette' 'Caprice'\n",
      " 'Camaro' 'Biscayne' 'Chevy' 'Impala' 'Lumina' 'Cobalt' 'Tahoe'\n",
      " 'Stylemaster' 'Traverse' 'SSR' 'Spark' 'Malibu' 'Silverado' 'Pickup'\n",
      " 'Express' 'Fleetmaster' 'Nova' 'Sonic' 'Trax' 'Astro' 'Vega'\n",
      " 'Confederate' '1500' 'Master' 'Cavalier' 'Volt' 'Avalanche' 'Panel'\n",
      " 'C30/K30' 'Blazer' 'S-10' '2500' 'Captiva' 'Apache' '3500' '3100'\n",
      " 'C10/K10' '150' 'Chevelle' 'Tracker' 'Trailblazer' 'Colorado' 'City'\n",
      " 'Equinox' 'Bel' 'Classic' 'Cruze' 'Prizm' 'Independence' 'Superior' '300'\n",
      " 'Crossfire' 'New' 'Cordoba' 'Concorde' 'LeBaron' 'Pacifica' 'Sebring'\n",
      " 'TC' 'Town' 'Aspen' 'Newport' 'Voyager' '200' 'LHS' '300C' '300M' 'PT'\n",
      " 'Prowler' 'Fifth' 'Caravan' 'Grand' 'Neon' '600' 'Ram' 'Ramcharger'\n",
      " 'Mini' 'Sprinter' 'Magnum' 'Polara' 'SRT' 'Stealth' 'Caliber' 'D150'\n",
      " 'Nitro' 'Diplomat' 'Challenger' 'Stratus' 'Avenger' 'W250' '400' 'D250'\n",
      " 'Journey' '880' 'Dart' 'Dakota' 'Viper' 'Intrepid' 'Coronet' 'Shadow'\n",
      " 'Charger' 'Durango' 'Focus' 'Explorer' 'Custom' 'Torino' 'EcoSport'\n",
      " 'Coupe' 'Mustang' 'Freestyle' 'Escape' 'Crown' 'C-Max' 'F-150' 'Sunliner'\n",
      " 'Flex' 'Falcon' 'Expedition' 'Fairlane' 'Parklane' 'Excursion' 'GT'\n",
      " 'Country' 'Galaxie' 'F-250' 'Fusion' 'Transit-350' 'Aerostar' 'E-Transit'\n",
      " 'E350' 'Freestar' 'Pinto' 'Bronco' 'Crestline' 'Utility' 'F100' 'GT40'\n",
      " 'Thunderbird' 'Five' 'F-350' 'XL' 'Maverick' 'LTD' 'Granada'\n",
      " 'Transit-250' 'Probe' 'Edge' 'Escort' 'E250' 'Club' 'Ranchero' 'Roadster'\n",
      " 'Fiesta' 'Ranger' 'F-450' 'Taurus' 'Customline' 'Transit-150' 'Deluxe'\n",
      " 'Shelby' 'Transit' 'Van' 'E150' 'Windstar' 'Vandura' 'Sierra' 'Yukon'\n",
      " 'Envoy' 'Savana' 'Canyon' 'Terrain' 'Sprint' 'Caballero' 'Acadia'\n",
      " 'HUMMER' 'Sonoma' 'Jimmy' 'Safari' 'Fit' 'S2000' 'Civic' 'CR-Z' 'Prelude'\n",
      " 'Crosstour' 'Pilot' 'Accord' 'Clarity' 'HR-V' 'Ridgeline' 'Odyssey'\n",
      " 'Element' 'Passport' 'CR-V' 'del' 'Insight' 'Kona' 'IONIQ' 'Santa'\n",
      " 'Equus' 'Elantra' 'Veloster' 'Palisade' 'Veracruz' 'Venue' 'Tucson'\n",
      " 'Genesis' 'Sonata' 'NEXO' 'Accent' 'XG350' 'Azera' 'Tiburon' 'Entourage'\n",
      " 'Q40' 'J30' 'Q50' 'Q70' 'QX4' 'EX35' 'G35x' 'G25' 'QX30' 'QX55' 'M45x'\n",
      " 'QX70' 'JX35' 'I30' 'Q60' 'M37' 'IPL' 'M30' 'G37' 'FX37' 'M35x' 'M37x'\n",
      " 'Q45' 'M45' 'QX50' 'FX45' 'G20' 'QX60' 'QX56' 'EX37' 'G35' 'FX35' 'M35'\n",
      " 'QX80' 'FX50' 'G37x' 'I35' 'M56' 'G25x' 'M56x' 'Q70L' 'M35h' 'XK8'\n",
      " 'F-TYPE' 'E-PACE' 'XJ' 'S-Type' 'XKR' 'I-PACE' 'XK' 'XE' 'Mark' 'X-Type'\n",
      " 'XKE' 'XF' 'XJR' 'XJ8' 'XJS' 'XJ6' 'F-PACE' 'Jeepster' 'J10' 'Scrambler'\n",
      " 'Gladiator' 'Patriot' 'CJ-5' 'Commander' 'Wrangler' 'Liberty' 'Wagoneer'\n",
      " 'Comanche' 'CJ-7' 'Cherokee' 'Compass' 'Renegade' 'Spectra5' 'Telluride'\n",
      " 'K900' 'Optima' 'Sportage' 'Niro' 'Sorento' 'Amanti' 'Soul' 'Cadenza'\n",
      " 'Forte' 'Sephia' 'Stinger' 'EV6' 'Rondo' 'Carnival' 'Rio5' 'Sedona'\n",
      " 'Seltos' 'K5' 'Rio' 'Borrego' 'Spectra' 'Range' 'LR2' 'LR4' 'Discovery'\n",
      " 'LR3' 'Defender' 'IS' 'GX' 'SC' 'LS' 'RX' 'NX' 'RC' 'ES' 'LX' 'UX' 'LC'\n",
      " 'HS' 'IS-F' 'CT' 'Aviator' 'Blackwood' 'Navigator' 'MKZ' 'Capri'\n",
      " 'Corsair' 'MKT' 'MKS' 'Nautilus' 'Zephyr' 'MKC' 'Continental' 'MKX'\n",
      " 'Cosmopolitan' 'Versailles' 'MazdaSpeed' 'CX-7' 'CX-9' 'Mazda6'\n",
      " 'MazdaSpeed3' 'B2000' 'CX-90' 'MX-5' 'CX-30' 'B3000' 'Millenia' '626'\n",
      " 'B2300' 'CX-3' 'MazdaSpeed6' 'Tribute' 'Mazda3' 'B2200' 'CX-5' 'Mazda2'\n",
      " 'Mazda5' 'RX-7' 'Protege' 'RX-8' 'CX-50' 'MPV' 'MX-30' 'MX-3' 'B4000'\n",
      " 'Protege5' 'CLA-Class' 'EQB' 'AMG' 'G' 'GLK-Class' 'GLC' 'CLS' 'Maybach'\n",
      " 'CL-Class' '230SL' 'SLK-Class' 'GLS' 'EQE' 'B-Class' 'E-Class' 'GLA'\n",
      " 'GLE' '280SE' '280SL' 'G-Class' 'GLB' 'C-Class' 'EQS' 'GL-Class' 'CLA'\n",
      " '190SL' 'SL-Class' 'SL' 'A-Class' 'M-Class' 'SLS' 'CLS-Class' 'SLR' 'SLC'\n",
      " 'R-Class' '450SL' 'GLA-Class' 'GLC-Class' 'Metris' 'CLK-Class' 'S-Class'\n",
      " 'GLE-Class' 'Lancer' 'Sigma' '3000GT' 'Outlander' 'Raider' 'Montero'\n",
      " 'Galant' 'Eclipse' 'Mirage' 'Endeavor' 'Diamante' 'i-MiEV' 'NV'\n",
      " 'Frontier' 'Xterra' 'Armada' 'Maxima' 'Cube' 'Versa' 'Murano' 'Altima'\n",
      " 'Leaf' '280ZX' '370Z' 'Rogue' 'Pathfinder' 'Juke' 'ARIYA' 'NV200' 'Kicks'\n",
      " 'Sentra' 'Titan' 'Z' 'Quest' 'GT-R' '240SX' '300ZX' '350Z' '200SX'\n",
      " 'Panamera' 'Cayenne' '718' '918' '914' 'Boxster' '912' '924' '930'\n",
      " 'Taycan' 'Macan' 'Carrera' 'Cayman' '928' '944' '911' '356' '968'\n",
      " 'ProMaster' 'Cargo' 'WRX' 'Impreza' 'Brat' 'STI' 'Solterra' 'XV' 'Baja'\n",
      " 'Ascent' 'BRZ' 'B9' 'Forester' 'Crosstrek' 'SVX' 'Outback' 'Tribeca'\n",
      " 'Legacy' 'Camry' 'RAV4' 'Highlander' 'Corolla' 'FJ' 'Cressida' 'Venza'\n",
      " 'C-HR' 'Yaris' '86' 'Previa' '4Runner' 'bZ4X' 'Corona' 'Sequoia' 'Mirai'\n",
      " 'Prius' 'Sienna' 'ECHO' 'Avalon' 'Matrix' 'Tercel' 'MR2' 'Celica' 'T100'\n",
      " 'Land' 'Tacoma' 'Supra' 'GR' 'Tundra' 'GR86' 'Eurovan' 'Touareg' 'Beetle'\n",
      " 'Routan' 'R32' 'Jetta' 'Atlas' 'Cabriolet' 'Golf' '1600' 'Eos' 'Microbus'\n",
      " 'Taos' 'Rabbit' 'Thing' 'Vanagon' 'Cabrio' 'Karmann' 'Tiguan' 'Corrado'\n",
      " 'CC' 'GTI' 'Arteon' 'Passat' 'ID.4' 'Phaeton' 'e-Golf' 'XC90' 'S70'\n",
      " 'XC40' 'S40' 'V60' 'V50' '240' 'S90' 'S60' 'XC70' 'S80' 'XC60' 'V90'\n",
      " 'C70' 'V40' 'V70' 'C40' 'C30']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Feature: model\n",
    "Approach:\n",
    "'''\n",
    "\n",
    "\n",
    "model_names = [m.split()[0] for m in data['model']]\n",
    "data['model'] = model_names\n",
    "\n",
    "print(\"Unique values:\", len(data['model'].unique()))\n",
    "print(\"Unique values of manufacturer:\", data['model'].unique())\n",
    "\n",
    "\n",
    "# for val in data['model'].unique():\n",
    "#     print(val)\n",
    "\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data['model'], prefix='model')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('model', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: year\n",
    "Approach: No missing values, one-hot-encode.\n",
    "'''\n",
    "\n",
    "year_values = data['year'].values\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = np.isnan(year_values).sum()\n",
    "total = len(year_values);\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "# calculate unique values\n",
    "unique_values = len(np.unique(year_values))\n",
    "print(\"Unique Values:\", unique_values)\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['year'], prefix='year')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('year', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: mileage\n",
    "Approach: Replace all missing values with the median and perform z-scaling.\n",
    "'''\n",
    "\n",
    "mileage_values = data['mileage'].values\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = np.isnan(mileage_values).sum()\n",
    "total = len(mileage_values);\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "mileage_values_without_nan = mileage_values[~np.isnan(mileage_values)]\n",
    "\n",
    "# calculate skewness\n",
    "skewness = skew(mileage_values_without_nan)\n",
    "print(\"Skewness of 'mileage':\", skewness)\n",
    "\n",
    "# Since distribution is heavily skewed towards the left (skewness < -1) => replace missing values with median\n",
    "\n",
    "# calculate median\n",
    "median = np.nanmedian(mileage_values)\n",
    "\n",
    "# fill missing values with median\n",
    "data['mileage'].fillna(median, inplace=True)\n",
    "\n",
    "# calculate mean and std\n",
    "mean = np.mean(mileage_values)\n",
    "std = np.std(mileage_values)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std)\n",
    "\n",
    "# Z-Scaling\n",
    "scaled_mileage_values = (mileage_values - mean) / std\n",
    "\n",
    "data['mileage'] = scaled_mileage_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "'''\n",
    "Feature: engine\n",
    "Approach:\n",
    "'''\n",
    "\n",
    "def extract_horsepower(engine_type):\n",
    "    if pd.isna(engine_type):\n",
    "        return np.nan\n",
    "    horsepower = re.search(r'(\\d+HP)', engine_type)\n",
    "    if horsepower:\n",
    "        return int(horsepower.group(1)[:-2])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def extract_liters(engine_type):\n",
    "    if pd.isna(engine_type):\n",
    "        return np.nan\n",
    "    liters = re.search(r'(\\d+\\.\\d+)L\\b', engine_type)\n",
    "    if liters:\n",
    "        return float(liters.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def is_turbo(engine_type):\n",
    "    if pd.isna(engine_type):\n",
    "        return 0\n",
    "    if \"Turbo\" in engine_type:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def extract_cylinders(engine_type):\n",
    "    if pd.isna(engine_type):\n",
    "        return np.nan\n",
    "    cylinders = re.search(r'\\b(I-\\d+|V\\d+)\\b', engine_type)\n",
    "    if cylinders:\n",
    "        cylinders_number = cylinders.group(0)[2:]\n",
    "        if cylinders_number.isdigit():  # Check if it's a valid integer\n",
    "            return int(cylinders_number)\n",
    "        else:\n",
    "            return np.nan  # Return NaN if it's not a valid integer\n",
    "    else:\n",
    "        return np.nan  # Return NaN if the pattern is not found\n",
    "\n",
    "\n",
    "def extract_injection_type(engine_type):\n",
    "    if pd.isna(engine_type):\n",
    "        return np.nan\n",
    "    injection_type = re.search(r'(MPFI|GDI|SPFI|PGM-FI|DI|SIDI|TFSI|FSI)', engine_type)\n",
    "    if injection_type:\n",
    "        return injection_type.group(0)\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def z_scaling(data):\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return (data - mean) / std\n",
    "\n",
    "def handle_missing_values(data):\n",
    "    for column in data.columns:\n",
    "        if data[column].dtype == 'object':  # Check if the column is non-numeric\n",
    "            mode_value = data[column].mode().iloc[0]  # Calculate mode\n",
    "            data[column].fillna(mode_value, inplace=True)  # Impute missing values with mode\n",
    "        else:\n",
    "            median_value = data[column].median()\n",
    "            data[column].fillna(median_value, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "data['horsepower'] = data['engine'].apply(extract_horsepower)\n",
    "data['liters'] = data['engine'].apply(extract_liters)\n",
    "data['turbo'] = data['engine'].apply(is_turbo)\n",
    "data['cylinders'] = data['engine'].apply(extract_cylinders)\n",
    "data['injection_type'] = data['engine'].apply(extract_injection_type)\n",
    "\n",
    "data = handle_missing_values(data)\n",
    "\n",
    "# Z-scaling\n",
    "# data['horsepower'] = z_scaling(data['horsepower'])\n",
    "# data['liters'] = z_scaling(data['liters'])\n",
    "# data['cylinders'] = z_scaling(data['cylinders'])\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['injection_type'], prefix='injection_type')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('injection_type', axis=1, inplace=True)\n",
    "data.drop('engine', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: transmission\n",
    "Approach: Change all transmisson to the right classification and extract the number of speeds in a new column. onehot encoding\n",
    "'''\n",
    "import re\n",
    "\n",
    "def get_transmission_info(transmission):\n",
    "    transmission_type = \"Unknown\"\n",
    "    number_of_speeds = \"Unknown\"\n",
    "    \n",
    "    if isinstance(transmission, str):\n",
    "        transmission = transmission.lower()\n",
    "        \n",
    "      # Use a regular expression to find the number of speeds\n",
    "       \n",
    "        if \"cvt\" in transmission or \"shiftronic\" in transmission or \"continuously\" in transmission:\n",
    "            transmission_type = \"Automatic Continuously Variable\"\n",
    "        \n",
    "        elif \"dual\" in transmission or \"dct\" in transmission:\n",
    "            transmission_type = \"Dual Clutch\"\n",
    "    \n",
    "        elif \"semi\" in transmission or \"autostick\" in transmission or \"paddle\" in transmission:\n",
    "            transmission_type = \"Semi-Automatic\"\n",
    "        \n",
    "        elif \"manual\" in transmission or \" m \" in transmission:\n",
    "            transmission_type = \"Manual\"\n",
    "        \n",
    "        elif \"automatic\" in transmission or \"auto\" in transmission or \"a/t\" in transmission or \" a \" in transmission or \" at\" in transmission :\n",
    "            transmission_type = \"Automatic\"\n",
    "            \n",
    "       \n",
    "    \n",
    "    \n",
    "        speeds = re.search(r'(\\d+)\\s*-?\\s*(spd|speed)', transmission)\n",
    "        if speeds:\n",
    "            number_of_speeds = speeds.group(1) + \"-speed\"\n",
    "            \n",
    "    return (transmission_type, number_of_speeds)\n",
    "\n",
    "data[\"transmission\"], data[\"number_of_speeds\"] = zip(*data[\"transmission\"].apply(get_transmission_info))\n",
    "\n",
    "print(\"Transmission unique values:\", len(data['transmission'].unique()))\n",
    "\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data['transmission'], prefix='transmission')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('transmission', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)\n",
    "\n",
    "print(\"Number of speeds unique values:\", len(data['number_of_speeds'].unique()))\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(data['number_of_speeds'], prefix='number_of_speeds')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('number_of_speeds', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: drivetrain\n",
    "Approach: Got 4 most dominant drivetrains, put rest and missing values in 'other'. One-hot-encoding.\n",
    "'''\n",
    "\n",
    "def process_drivetrain(drivetrain):    \n",
    "    if isinstance(drivetrain, str):\n",
    "        drivetrain = drivetrain.lower()\n",
    "        if \"front\" in drivetrain or \"fwd\" in drivetrain:\n",
    "            return \"Front-Wheel Drive\"\n",
    "        elif \"all\" in drivetrain or \"awd\" in drivetrain:\n",
    "            return \"All-Wheel Drive\"\n",
    "        elif \"four\" in drivetrain or \"4\" in drivetrain:\n",
    "            return \"Four-Wheel Drive\"\n",
    "        elif \"rear\" in drivetrain or \"rwd\" in drivetrain:\n",
    "            return \"Rear-Wheel Drive\"\n",
    "        else:\n",
    "            return 'other'\n",
    "    else:\n",
    "        return 'other'\n",
    "        \n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['drivetrain'].isnull().sum()\n",
    "total = len(data['drivetrain'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "data['drivetrain'] = data['drivetrain'].apply(process_drivetrain)\n",
    "\n",
    "# calculate 'not' missing values which will be added to others \n",
    "other_count = (data['drivetrain'] == 'other').sum()\n",
    "print(\"'Other' count excluding missing:\", other_count - missing_count)\n",
    "\n",
    "# calculate unique values\n",
    "unique_values = len(data['drivetrain'].unique())\n",
    "print(\"Unique Values:\", unique_values)\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['drivetrain'], prefix='drivetrain')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('drivetrain', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: fuel_type\n",
    "Approach: Got the top 6 fuel types. Put the rest and missing values into 'other'. One-hot-encoding.\n",
    "'''\n",
    "\n",
    "def process_fuel_type(fuel):\n",
    "        if isinstance(fuel, str):\n",
    "            fuel = fuel.lower()\n",
    "        else:\n",
    "            fuel = 'other'\n",
    "        if fuel in fuel_types:\n",
    "            return fuel\n",
    "        else:\n",
    "            return 'other'\n",
    "\n",
    "        \n",
    "# calculate missing values\n",
    "missing_count = data['fuel_type'].isnull().sum()\n",
    "total = len(data['fuel_type'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "fuel_types = ['gasoline', 'hybrid', 'diesel', 'e85 flex fuel', 'electric', 'b']\n",
    "\n",
    "data['fuel_type'] = data['fuel_type'].apply(process_fuel_type)\n",
    "\n",
    "# calculate 'not' missing values which will be added to others \n",
    "other_count = (data['fuel_type'] == 'other').sum()\n",
    "print(\"'Other' count excluding missing:\", other_count - missing_count)\n",
    "\n",
    "# calculate unique values\n",
    "unique_values = len(data['fuel_type'].unique())\n",
    "print(\"Unique Values:\", unique_values)\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['fuel_type'], prefix='fuel_type')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('fuel_type', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: mpg\n",
    "Approach: Replace all missing values with the median and perform z-scaling.\n",
    "'''\n",
    "\n",
    "def calculate_middle_value(mpg_values):\n",
    "    updated_values = []\n",
    "    for value in mpg_values:\n",
    "        if isinstance(value, str) and '-' in value:\n",
    "            start, end = map(float, value.split('-'))\n",
    "            updated_values.append((start + end) / 2)\n",
    "        else:\n",
    "            updated_values.append(value)\n",
    "    return pd.Series(updated_values)\n",
    "\n",
    "\n",
    "mpg_values = data['mpg'].values\n",
    "\n",
    "# Update range with middle value\n",
    "mpg_values = calculate_middle_value(data['mpg'])\n",
    "\n",
    "# Convert to numeric, handle errors by coercing to NaN\n",
    "mpg_values = pd.to_numeric(mpg_values, errors='coerce')\n",
    "\n",
    "# Replace missing values with median\n",
    "mean = np.nanmean(mpg_values)\n",
    "mpg_values.fillna(mean, inplace=True)\n",
    "\n",
    "# Calculate skewness\n",
    "skewness = skew(mpg_values)\n",
    "print(\"Skewness of 'mpg':\", skewness)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.nanmean(mpg_values)\n",
    "std = np.nanstd(mpg_values)\n",
    "\n",
    "# Z-Scaling\n",
    "scaled_mpg_values = (mpg_values - mean) / std\n",
    "\n",
    "data['mpg'] = scaled_mpg_values\n",
    "\n",
    "\n",
    "# Ensure all missing values are replaced with mean\n",
    "missing_count = np.isnan(data['mpg']).sum()\n",
    "if missing_count > 0:\n",
    "    mean_mpg = data['mpg'].mean()\n",
    "    print(\"Replacing remaining missing values with mean:\", mean_mpg)\n",
    "    data['mpg'].fillna(mean_mpg, inplace=True)\n",
    "\n",
    "print(\"Number of missing values in 'mpg' column:\", data['mpg'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: exterior_color\n",
    "Approach: Categorize top 30 colors, others in 'other', one-hot-encoding.\n",
    "'''\n",
    "\n",
    "# Create a list of words from the 'exterior_color' column\n",
    "words = [word.lower() for item in data['exterior_color'] for word in str(item).split()]\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Sort the Counter dictionary by frequency in descending order and limit to top 30\n",
    "sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "top_30_colors = [word for word, _ in sorted_freq]\n",
    "\n",
    "def replace_words(item):\n",
    "    words = str(item).split()\n",
    "    for word in words:\n",
    "        if word.lower() in top_30_colors:\n",
    "            return word.lower()  \n",
    "    return 'other'\n",
    "\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['exterior_color'].isnull().sum()\n",
    "total = len(data['exterior_color'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "data['exterior_color'] = data['exterior_color'].apply(replace_words)\n",
    "\n",
    "# calculate 'not' missing values which will be added to others \n",
    "other_count = (data['exterior_color'] == 'other').sum()\n",
    "print(\"'Other' count excluding missing:\", other_count - missing_count)\n",
    "\n",
    "# calculate unique values\n",
    "unique_values = len(data['exterior_color'].unique())\n",
    "print(\"Unique Values:\", unique_values)\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['exterior_color'], prefix='exterior_color')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('exterior_color', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: interior_color\n",
    "Approach: Categorize top 20 colors, others in 'other', one-hot-encoding.\n",
    "'''\n",
    "\n",
    "# Create a list of words from the 'interior_color' column\n",
    "words = [word.lower() for item in data['interior_color'] for word in str(item).split()]\n",
    "\n",
    "# Count the frequency of each word\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Sort the Counter dictionary by frequency in descending order and limit to top 20\n",
    "sorted_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "top_20_colors = [word for word, _ in sorted_freq]\n",
    "\n",
    "def replace_words(item):\n",
    "    words = str(item).split()\n",
    "    for word in words:\n",
    "        if word.lower() in top_20_colors:\n",
    "            return word.lower()  \n",
    "    return 'other'\n",
    "\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['interior_color'].isnull().sum()\n",
    "total = len(data['interior_color'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "data['interior_color'] = data['interior_color'].apply(replace_words)\n",
    "\n",
    "# calculate 'not' missing values which will be added to others \n",
    "other_count = (data['interior_color'] == 'other').sum()\n",
    "print(\"'Other' count excluding missing:\", other_count - missing_count)\n",
    "\n",
    "# calculate unique values\n",
    "unique_values = len(data['interior_color'].unique())\n",
    "print(\"Unique Values:\", unique_values)\n",
    "\n",
    "# One-hot-encoding\n",
    "one_hot_encoded = pd.get_dummies(data['interior_color'], prefix='interior_color')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('interior_color', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: accidents_or_damage\n",
    "Approach: Split into 3 new columns (true, false, missing)\n",
    "'''\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['accidents_or_damage'].isnull().sum()\n",
    "total = len(data['accidents_or_damage'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100,\"%)\")\n",
    "\n",
    "# change nan to a value so it could be categorized\n",
    "data['accidents_or_damage'] = data['accidents_or_damage'].fillna('missing')\n",
    "\n",
    "# one-hot-encode\n",
    "one_hot_encoded = pd.get_dummies(data['accidents_or_damage'], prefix='accidents_or_damage')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('accidents_or_damage', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: one_owner\n",
    "Approach: Split into 3 new columns (true, false, missing)\n",
    "'''\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['one_owner'].isnull().sum()\n",
    "total = len(data['one_owner'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100,\"%)\")\n",
    "\n",
    "# change nan to a value so it could be categorized\n",
    "data['one_owner'] = data['one_owner'].fillna('missing')\n",
    "\n",
    "# one-hot-encode\n",
    "one_hot_encoded = pd.get_dummies(data['one_owner'], prefix='one_owner')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('one_owner', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: personal_use_only\n",
    "Approach: Split into 3 new columns (true, false, missing)\n",
    "'''\n",
    "\n",
    "# calculate missing values\n",
    "missing_count = data['personal_use_only'].isnull().sum()\n",
    "total = len(data['personal_use_only'])\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100,\"%)\")\n",
    "\n",
    "# change nan to a value so it could be categorized\n",
    "data['personal_use_only'] = data['personal_use_only'].fillna('missing')\n",
    "\n",
    "# one-hot-encode\n",
    "one_hot_encoded = pd.get_dummies(data['personal_use_only'], prefix='personal_use_only')\n",
    "one_hot_encoded = one_hot_encoded.astype(int)\n",
    "data.drop('personal_use_only', axis=1, inplace=True)\n",
    "data = pd.concat([data, one_hot_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: seller_name\n",
    "Approach: Drop feature\n",
    "'''\n",
    "\n",
    "data.drop('seller_name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: seller_rating\n",
    "Approach: Replace all missing values with 0 and perform z-scaling.\n",
    "'''\n",
    "\n",
    "seller_rating_values = data['seller_rating'].values\n",
    "\n",
    "# Calculate missing values rate\n",
    "missing_count = np.isnan(seller_rating_values).sum()\n",
    "total = len(seller_rating_values)\n",
    "print(\"Missing values:\", missing_count, \"(\", (missing_count / total) * 100, \"%)\")\n",
    "\n",
    "# Fill missing values with 0 (not calculating skewness as we're not using it for imputation)\n",
    "data['seller_rating'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean = np.mean(seller_rating_values)\n",
    "std = np.std(seller_rating_values)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n",
    "\n",
    "# Perform z-scaling\n",
    "scaled_seller_rating_values = (seller_rating_values - mean) / std\n",
    "\n",
    "data['seller_rating'] = scaled_seller_rating_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: driver_rating\n",
    "Approach: Replace all missing values with the median and perform z-scaling.\n",
    "'''\n",
    "\n",
    "driver_rating_values = data['driver_rating'].values\n",
    "\n",
    "# calculate missing values rate\n",
    "missing_count = np.isnan(driver_rating_values).sum()\n",
    "total = len(driver_rating_values);\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count / total) * 100,\"%)\")\n",
    "\n",
    "driver_rating_values_without_nan = driver_rating_values[~np.isnan(driver_rating_values)]\n",
    "\n",
    "# calculate skewness\n",
    "skewness = skew(driver_rating_values_without_nan)\n",
    "print(\"Skewness of 'driver_rating_values':\", skewness)\n",
    "\n",
    "# Since distribution is heavily skewed towards the left (skewness < -1) => replace missing values with median\n",
    "\n",
    "# calculate median\n",
    "median = np.nanmedian(driver_rating_values)\n",
    "\n",
    "# fill missing values with median\n",
    "data['driver_rating'].fillna(median, inplace=True)\n",
    "\n",
    "# calculate mean and std\n",
    "mean = np.mean(driver_rating_values)\n",
    "std = np.std(driver_rating_values)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Median:\", median)\n",
    "print(\"Standard Deviation:\", std)\n",
    "\n",
    "# (x - mean) / std\n",
    "scaled_driver_rating_values = (driver_rating_values - mean) / std\n",
    "\n",
    "data['driver_rating'] = scaled_driver_rating_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Feature: driver_reviews_num\n",
    "Approach: No missing values, perform z-scaling.\n",
    "'''\n",
    "\n",
    "driver_reviews_values = data['driver_reviews_num'].values\n",
    "\n",
    "print(len(np.unique(driver_reviews_values)))\n",
    "\n",
    "# calculate missing values rate\n",
    "missing_count = np.isnan(driver_reviews_values).sum()\n",
    "total = len(driver_reviews_values);\n",
    "print(\"Missing values:\", missing_count, \"(\" , (missing_count/total) * 100,\"%)\")\n",
    "\n",
    "# calculate mean and std\n",
    "mean = np.mean(driver_reviews_values)\n",
    "std = np.std(driver_reviews_values)\n",
    "\n",
    "print(\"Mean:\", mean)\n",
    "print(\"Standard Deviation:\", std)\n",
    "\n",
    "# (x - mean) / std\n",
    "scaled_driver_reviews_values = (driver_reviews_values - mean) / std\n",
    "\n",
    "data['driver_reviews_num'] = scaled_driver_reviews_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Name: \n",
    "Feature: price_drop\n",
    "Approach: Drop feature\n",
    "'''\n",
    "\n",
    "data.drop('price_drop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for percentile in range(0, 101, 1):\n",
    "#     value_at_percentile = data['price'].quantile(percentile / 100)\n",
    "#     print(f\"Value at {percentile}th percentile:\", value_at_percentile)\n",
    "\n",
    "# print()\n",
    "\n",
    "low = data['price'].quantile(0.01)\n",
    "high = data['price'].quantile(0.99)\n",
    "\n",
    "print(\"Value at 1th percentile:\", low)\n",
    "print(\"Value at 99th percentile:\", high)\n",
    "\n",
    "data['price'] = data['price'].clip(3000, 120000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    if data[column].dtype in ['int64', 'float64']:  # Check if column contains numeric values\n",
    "        if data[column].nunique() > 2:  # Check if column has more than 2 unique values (excluding 0's and 1's)\n",
    "            print(f\"Column Name: {column}\")\n",
    "            print(f\"Max Value before transformation: {data[column].max()}\")\n",
    "            print(f\"Min Value before transformation: {data[column].min()}\")\n",
    "            \n",
    "            # Apply transformations\n",
    "            if data[column].dtype == 'float64' and column != 'price':\n",
    "                data[column] = data[column].clip(lower=-2, upper=2)\n",
    "            \n",
    "            print(f\"Max Value after transformation: {data[column].max()}\")\n",
    "            print(f\"Min Value after transformation: {data[column].min()}\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in dataset:\", data.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training Models<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range (0, 1000):\n",
    "#     print(data.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['price'])\n",
    "y = data['price']\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>KNN<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear Regression<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Decision Tree<h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forests<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
